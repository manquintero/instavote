name: Python application

on:
  workflow_call:
    inputs:
      MICROSERVICE:
        description: "Component to build"
        type: string
        required: true

env:
  PYTHON_VERSION: "3.11"

jobs:
  build:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ${{ github.workspace }}/${{ inputs.MICROSERVICE }}
    outputs:
      runtime-version: ${{ steps.set-result.outputs.result }}

    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest pytest-cov
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Lint with flake8
        run: |
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 . --count --max-complexity=10 --max-line-length=127 --statistics

      - name: Test with pytest
        id: pytest
        run: |
          test_results="${PWD}/test-results.xml"
          pytest --junit-xml "${test_results}"

          # Generate Output
          echo "report_paths=${test_results}" | tee -a $GITHUB_OUTPUT
        env:
          PYTHONPATH: ${{ github.workspace }}/${{ inputs.MICROSERVICE }}

      - name: Publish Test Report
        uses: mikepenz/action-junit-report@v4
        if: success() || failure() # always run even if the previous step fails
        with:
          report_paths: ${{ steps.pytest.outputs.report_paths }}
          detailed_summary: true
          include_passed: true
          job_name: 'Test Report (${{ inputs.MICROSERVICE }})'

      - name: Generate Coverage
        id: coverage
        run: |
          pytest --cov --cov-report=xml

          # Expose Coverage path
          path=$(realpath --relative-to=${{ github.workspace }} coverage.xml)
          echo "path=${path}" | tee -a $GITHUB_OUTPUT
        env:
          PYTHONPATH: ${{ github.workspace }}/${{ inputs.MICROSERVICE }}

      - name: Publish Coverage
        uses: 5monkeys/cobertura-action@master
        with:
          path: ${{ steps.coverage.outputs.path }}
          minimum_coverage: 70
          fail_below_threshold: true
          show_line: true
          show_branch: true

      - name: Check if Dirty
        uses: ./.github/actions/check-dirty-repo

      - name: Export Runtime Version
        uses: actions/github-script@v6
        id: set-result
        with:
          script: return "${{ env.PYTHON_VERSION }}"
          result-encoding: string

  container:
    needs:
      - build
    uses: ./.github/workflows/container-build.yaml
    with:
      MICROSERVICE: ${{ inputs.MICROSERVICE }}
      DOCKERFILE: Dockerfile-scratch
      BUILD-ARGS: "PYTHON_VERSION=${{ needs.build.outputs.runtime-version }}"

  smoke-test:
    needs: [container]
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ${{ github.workspace }}/${{ inputs.MICROSERVICE }}
    env:
      PORT: 8080
    steps:
      - uses: actions/checkout@v4

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Bring up the Application
        run: docker compose up -d --quiet-pull
        env:
          REPOSITORY_OWNER: ${{ github.repository_owner }}

      - name: Run Smoke Test
        run: |
          set -x; WAIT_TIME=0; MAX_RETRIES=5;
          until [ "${WAIT_TIME}" -eq "${MAX_RETRIES}" ] || curl -q ${TEST_URL} 2>/dev/null; do
            echo "Waiting for ${TEST_URL} to come up"
            WAIT_TIME=$(( WAIT_TIME + 1 ))
            sleep "${WAIT_TIME}"
          done;
          test "${WAIT_TIME}" -lt "${MAX_RETRIES}";
        env:
          TEST_URL: "localhost:${{ env.PORT }}"
